{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915cd9be",
   "metadata": {
    "papermill": {
     "duration": 0.00257,
     "end_time": "2025-03-27T20:26:40.718318",
     "exception": false,
     "start_time": "2025-03-27T20:26:40.715748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimize Direct Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74b92ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:26:40.723640Z",
     "iopub.status.busy": "2025-03-27T20:26:40.723310Z",
     "iopub.status.idle": "2025-03-27T20:26:49.538825Z",
     "shell.execute_reply": "2025-03-27T20:26:49.537807Z"
    },
    "papermill": {
     "duration": 8.819652,
     "end_time": "2025-03-27T20:26:49.540351",
     "exception": false,
     "start_time": "2025-03-27T20:26:40.720699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting calflops\r\n",
      "  Downloading calflops-0.3.2-py3-none-any.whl.metadata (28 kB)\r\n",
      "Collecting lightning\r\n",
      "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\r\n",
      "Collecting pyts==0.12.0\r\n",
      "  Downloading pyts-0.12.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.13.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.4.2)\r\n",
      "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (0.60.0)\r\n",
      "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from calflops) (1.2.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from calflops) (0.29.0)\r\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from calflops) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\r\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\r\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.12.0)\r\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\r\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\r\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\r\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (5.9.5)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (0.4.5)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.12)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (3.17.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.48.0->pyts==0.12.0) (0.43.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (2.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyts==0.12.0) (3.5.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->calflops) (1.3.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.6)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->calflops) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.5->pyts==0.12.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.5->pyts==0.12.0) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.5->pyts==0.12.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.5->pyts==0.12.0) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.5->pyts==0.12.0) (2024.2.0)\r\n",
      "Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading calflops-0.3.2-py3-none-any.whl (29 kB)\r\n",
      "Downloading lightning-2.5.1-py3-none-any.whl (818 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyts, lightning, calflops\r\n",
      "Successfully installed calflops-0.3.2 lightning-2.5.1 pyts-0.12.0\r\n",
      "Collecting tsai==0.3.9\r\n",
      "  Downloading tsai-0.3.9-py3-none-any.whl.metadata (16 kB)\r\n",
      "Downloading tsai-0.3.9-py3-none-any.whl (324 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tsai\r\n",
      "Successfully installed tsai-0.3.9\r\n"
     ]
    }
   ],
   "source": [
    "!pip install calflops lightning pyts==0.12.0\n",
    "!pip install --no-deps tsai==0.3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2ed9e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:26:49.548228Z",
     "iopub.status.busy": "2025-03-27T20:26:49.547988Z",
     "iopub.status.idle": "2025-03-27T20:27:04.817574Z",
     "shell.execute_reply": "2025-03-27T20:27:04.816691Z"
    },
    "papermill": {
     "duration": 15.275332,
     "end_time": "2025-03-27T20:27:04.819205",
     "exception": false,
     "start_time": "2025-03-27T20:26:49.543873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.exceptions import OptunaError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "from calflops import calculate_flops\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, Logger\n",
    "from lightning.pytorch.utilities import disable_possible_user_warnings, rank_zero_only\n",
    "from fastai.imports import noop\n",
    "from fastai.layers import AdaptiveConcatPool1d\n",
    "from tsai.models.layers import Conv, Concat, Norm, ConvBlock, GAP1d, Add\n",
    "from typing import Literal\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "\n",
    "class BaseLightningModel(L.LightningModule):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self._val_loss = 0.0\n",
    "        self._test_loss = 0.0\n",
    "        self._val_batches = 0\n",
    "        self._test_batches = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        self._val_loss += self.val_criterion(output, target).item()\n",
    "        self._val_batches += 1\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        self._test_loss += self.val_criterion(output, target).item()\n",
    "        self._test_batches += 1\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "            batch, y = batch\n",
    "            return self(batch), y\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.config.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.config.lr)\n",
    "        elif self.config.optimizer == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.config.optimizer}\")\n",
    "        if self.config.use_one_cycle:\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, pct_start=0.25,\n",
    "                                                            total_steps=self.trainer.estimated_stepping_batches)\n",
    "            opt_config = {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                }\n",
    "            }\n",
    "            return opt_config\n",
    "        return optimizer\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        callbacks = []\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.config.patience,\n",
    "            min_delta=self.config.min_delta,\n",
    "            verbose=True\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "        if self.config.modelOutput is not None:\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                dirpath=self.config.modelOutput,\n",
    "                filename=\"best_{epoch:02d}-{val_loss:.3f}\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                save_last=True\n",
    "            )\n",
    "            callbacks.append(checkpoint)\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "        callbacks.append(lr_monitor)\n",
    "        return callbacks\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Compute and log validation metrics at the end of the validation epoch.\"\"\"\n",
    "        val_loss = self._val_loss / self._val_batches\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self._val_loss = 0.0\n",
    "        self._val_batches = 0\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"Compute and log test metrics at the end of the test epoch.\"\"\"\n",
    "        test_loss = self._test_loss / self._test_batches\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        self._test_loss = 0.0\n",
    "        self._test_batches = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_act(activation: Literal[\"relu\", \"leakyrelu\", \"mish\", \"silu\", \"hardswish\", \"gelu\", \"celu\", \"elu\"] = \"relu\"):\n",
    "        activation = activation.lower()\n",
    "        if activation == \"relu\":\n",
    "            return torch.nn.ReLU\n",
    "        elif activation == \"leakyrelu\":\n",
    "            return torch.nn.LeakyReLU\n",
    "        elif activation == \"mish\":\n",
    "            return torch.nn.Mish\n",
    "        elif activation == \"silu\":  # Swish\n",
    "            return torch.nn.SiLU\n",
    "        elif activation == \"hardswish\":\n",
    "            return torch.nn.Hardswish\n",
    "        elif activation == \"gelu\":\n",
    "            return torch.nn.GELU\n",
    "        elif activation == \"celu\":\n",
    "            return torch.nn.CELU\n",
    "        elif activation == \"elu\":\n",
    "            return torch.nn.ELU\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "\n",
    "class BaseClassifier(BaseLightningModel):\n",
    "    def __init__(self, criterion, val_criterion, optimizer, lr, patience, min_delta, checkpoint_dir, use_one_cycle):\n",
    "        super().__init__()\n",
    "        self.criterion = criterion if criterion is not None else nn.BCEWithLogitsLoss()\n",
    "        self.val_criterion = val_criterion if val_criterion is not None else criterion\n",
    "        self.optimizer_name = optimizer\n",
    "        self.lr = lr\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.use_one_cycle = use_one_cycle\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        elif self.optimizer_name == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.optimizer_name}\")\n",
    "        if self.use_one_cycle:\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, pct_start=0.25,\n",
    "                                                            total_steps=self.trainer.estimated_stepping_batches)\n",
    "            opt_config = {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                }\n",
    "            }\n",
    "            return opt_config\n",
    "        return optimizer\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        callbacks = []\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.patience,\n",
    "            min_delta=self.min_delta,\n",
    "            verbose=True\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "        if self.checkpoint_dir is not None:\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                dirpath=self.checkpoint_dir,\n",
    "                filename=\"best_{epoch:02d}-{val_loss:.3f}\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                save_last=True\n",
    "            )\n",
    "            callbacks.append(checkpoint)\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "        callbacks.append(lr_monitor)\n",
    "        return callbacks\n",
    "\n",
    "\n",
    "class XceptionModulePlus(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=40, kss=None, bottleneck=True, coord=False, separable=True, norm='Batch',\n",
    "                 bn_1st=True, act=nn.ReLU, act_kwargs=None, norm_act=False):\n",
    "        super().__init__()\n",
    "        act_kwargs = {} if act_kwargs is None else act_kwargs\n",
    "        if kss is None:\n",
    "            kss = [ks // (2 ** i) for i in range(3)]\n",
    "        kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]  # ensure odd kss for padding='same'\n",
    "        self.bottleneck = Conv(ni, nf, 1, coord=coord, bias=False) if bottleneck else noop\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(len(kss)):\n",
    "            self.convs.append(Conv(nf if bottleneck else ni, nf, kss[i], coord=coord, separable=separable, bias=False))\n",
    "        self.mp_conv = nn.Sequential(*[nn.MaxPool1d(3, stride=1, padding=1), Conv(ni, nf, 1, coord=coord, bias=False)])\n",
    "        self.concat = Concat()\n",
    "        _norm_act = []\n",
    "        if act is not None:\n",
    "            _norm_act.append(act(**act_kwargs))\n",
    "        _norm_act.append(Norm(nf * 4, norm=norm, zero_norm=False))\n",
    "        if bn_1st:\n",
    "            _norm_act.reverse()\n",
    "        self.norm_act = noop if not norm_act else _norm_act[0] if act is None else nn.Sequential(*_norm_act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.concat([l(x) for l in self.convs] + [self.mp_conv(input_tensor)])\n",
    "        return self.norm_act(x)\n",
    "\n",
    "\n",
    "class XceptionBlockPlus(nn.Module):\n",
    "    def __init__(self, ni, nf, residual=True, coord=False, norm='Batch', act=nn.ReLU, act_kwargs=None, dropout=0., **kwargs):\n",
    "        super().__init__()\n",
    "        act_kwargs = {} if act_kwargs is None else act_kwargs\n",
    "        self.residual = residual\n",
    "        self.xception, self.shortcut, self.act = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            if self.residual and (i - 1) % 2 == 0:\n",
    "                self.shortcut.append(\n",
    "                    Norm(n_in, norm=norm) if n_in == n_out else\n",
    "                    ConvBlock(n_in, n_out * 4 * 2, 1, coord=coord, bias=False, norm=norm, act=None, dropout=dropout)\n",
    "                )\n",
    "                self.act.append(act(**act_kwargs))\n",
    "            n_out = nf * 2 ** i\n",
    "            n_in = ni if i == 0 else n_out * 2\n",
    "            self.xception.append(XceptionModulePlus(n_in, n_out, coord=coord, norm=norm,\n",
    "                                                    act=act if self.residual and (i - 1) % 2 == 0 else None, **kwargs))\n",
    "        self.add = Add()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for i in range(4):\n",
    "            x = self.xception[i](x)\n",
    "            if self.residual and (i + 1) % 2 == 0:\n",
    "                res = x = self.act[i // 2](self.add(x, self.shortcut[i // 2](res)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class XceptionTimePlus(BaseClassifier):\n",
    "    def __init__(\n",
    "            self,\n",
    "            c_in,\n",
    "            c_out,\n",
    "            nf=16,\n",
    "            coord=False,\n",
    "            norm=\"Batch\",\n",
    "            concat_pool=False,\n",
    "            adaptive_size=48,\n",
    "            activation=\"relu\",\n",
    "            dropout=0.0,\n",
    "            ks=40,\n",
    "            bottleneck=True,\n",
    "            bn_1st=True,\n",
    "            norm_act=False,\n",
    "            width=16,\n",
    "            input_norm=True,\n",
    "            window_size=None,\n",
    "            norm_weights=False,\n",
    "            # BaseClassifier params\n",
    "            criterion=None,\n",
    "            val_criterion=None,\n",
    "            optimizer=\"adam\",\n",
    "            lr=1e-3,\n",
    "            patience=5,\n",
    "            min_delta=0.0,\n",
    "            checkpoint_dir=None,\n",
    "            use_one_cycle=False\n",
    "    ):\n",
    "        super().__init__(criterion, val_criterion, optimizer, lr, patience, min_delta, checkpoint_dir, use_one_cycle)\n",
    "\n",
    "        # params\n",
    "        act = self.get_act(activation)  # nn.ReLU\n",
    "\n",
    "        # input standardization\n",
    "        if input_norm:\n",
    "            assert window_size is not None, \"window_size must be provided if input_norm is True\"\n",
    "            self.in_norm = nn.LayerNorm([c_in, window_size], elementwise_affine=norm_weights, bias=norm_weights)\n",
    "        else:\n",
    "            self.in_norm = nn.Identity()\n",
    "\n",
    "        # Backbone\n",
    "        self.backbone = XceptionBlockPlus(c_in, nf, coord=coord, norm=norm, act=act, dropout=dropout, ks=ks,\n",
    "                                          bottleneck=bottleneck, bn_1st=bn_1st, norm_act=norm_act)\n",
    "        # Head\n",
    "        gap1 = AdaptiveConcatPool1d(adaptive_size) if adaptive_size and concat_pool else nn.AdaptiveAvgPool1d(adaptive_size) if adaptive_size else noop\n",
    "        mult = 2 if adaptive_size and concat_pool else 1\n",
    "        conv1x1_1 = ConvBlock(nf * 32 * mult, nf * width * mult, 1, coord=coord, norm=norm)\n",
    "        conv1x1_2 = ConvBlock(nf * width * mult, nf * width // 2 * mult, 1, coord=coord, norm=norm)\n",
    "        conv1x1_3 = ConvBlock(nf * width // 2 * mult, c_out, 1, coord=coord, norm=norm)\n",
    "        gap2 = GAP1d(1)\n",
    "        lin = nn.Linear(c_out, c_out)  # Added by me to avoid ReLU preventing negative values\n",
    "        self.head = nn.Sequential(gap1, conv1x1_1, conv1x1_2, conv1x1_3, gap2, lin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_norm(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray = None,\n",
    "            window_size: int = 1,\n",
    "            stride: int = 1,\n",
    "    ):\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.number_of_windows = ((len(X) - window_size) // stride) + 1\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).squeeze() if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get window\n",
    "        start_idx = idx * self.stride\n",
    "        end_idx = start_idx + self.window_size\n",
    "\n",
    "        # apply instance normalization\n",
    "        window = self.X[start_idx:end_idx]  # (window_size, n_features)\n",
    "\n",
    "        # change to channels first format\n",
    "        window = window.permute(1, 0)\n",
    "\n",
    "        # get label and return\n",
    "        if self.y is not None:\n",
    "            return window, self.y[end_idx - 1]\n",
    "        return window\n",
    "\n",
    "\n",
    "def get_weighted_sampler(dataset: TSDataset, num_samples: int):\n",
    "    global_labels = dataset.y.numpy().max(axis=1).astype(int)\n",
    "    unique, counts = np.unique(global_labels, return_counts=True)\n",
    "    class_weights = 1.0 / counts\n",
    "    sample_weights = class_weights[global_labels]\n",
    "    sample_weights = sample_weights[dataset.window_size - 1::dataset.stride]\n",
    "    return WeightedRandomSampler(sample_weights, num_samples, replacement=True)\n",
    "\n",
    "\n",
    "def get_class_weights(labels: np.ndarray):\n",
    "    pos_counts = labels.sum(axis=0)\n",
    "    neg_counts = len(labels) - pos_counts\n",
    "    class_weights = neg_counts / pos_counts\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return torch.tensor(class_weights, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155eb337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:29:08.95813Z",
     "start_time": "2025-03-22T15:29:08.947032Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T20:27:04.827095Z",
     "iopub.status.busy": "2025-03-27T20:27:04.826701Z",
     "iopub.status.idle": "2025-03-27T20:27:04.830386Z",
     "shell.execute_reply": "2025-03-27T20:27:04.829765Z"
    },
    "papermill": {
     "duration": 0.008771,
     "end_time": "2025-03-27T20:27:04.831622",
     "exception": false,
     "start_time": "2025-03-27T20:27:04.822851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_samples_per_epoch = 2_097_152  # 2048 steps\n",
    "epochs = 10\n",
    "batch_size = 1024\n",
    "accumulate_grad_batches = 2  # virtual batch size of 2048\n",
    "split = 264_960\n",
    "file_dir = \"/kaggle/working\"\n",
    "\n",
    "seed = 42\n",
    "in_channels = 6\n",
    "out_channels = 6\n",
    "eval_steps = 1024  # val_check_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649cca08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:29:32.827606Z",
     "start_time": "2025-03-22T15:29:19.93589Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T20:27:04.838299Z",
     "iopub.status.busy": "2025-03-27T20:27:04.838096Z",
     "iopub.status.idle": "2025-03-27T20:27:33.695704Z",
     "shell.execute_reply": "2025-03-27T20:27:33.694795Z"
    },
    "papermill": {
     "duration": 28.862448,
     "end_time": "2025-03-27T20:27:33.697132",
     "exception": false,
     "start_time": "2025-03-27T20:27:04.834684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [75.98615178 76.00953507 75.47281678 75.46128577 76.02624612 75.9719617 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7099201, 6), (7099201, 6), (264960, 6), (264960, 6))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/esa-mission-1-train-dataset/84_months.train.csv\")\n",
    "\n",
    "target_channels = [f\"channel_{i}\" for i in range(41, 47)]\n",
    "label_cols = [\"is_anomaly_\" + col for col in target_channels]\n",
    "\n",
    "train_array = df_train[target_channels].iloc[:-split].values.astype(np.float32)\n",
    "val_array = df_train[target_channels].iloc[-split:].values.astype(np.float32)\n",
    "train_labels = df_train[label_cols].iloc[:-split].values.astype(np.float32).clip(0., 1.)\n",
    "val_labels = df_train[label_cols].iloc[-split:].values.astype(np.float32).clip(0., 1.)\n",
    "\n",
    "class_weights = get_class_weights(train_labels)\n",
    "\n",
    "train_array.shape, train_labels.shape, val_array.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8adac50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:29:38.445387Z",
     "start_time": "2025-03-22T15:29:38.419569Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T20:27:33.705222Z",
     "iopub.status.busy": "2025-03-27T20:27:33.704963Z",
     "iopub.status.idle": "2025-03-27T21:34:00.777132Z",
     "shell.execute_reply": "2025-03-27T21:34:00.776479Z"
    },
    "papermill": {
     "duration": 3987.077727,
     "end_time": "2025-03-27T21:34:00.778550",
     "exception": false,
     "start_time": "2025-03-27T20:27:33.700823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  18.88 K \n",
      "fwd MACs:                                                               907.05 KMACs\n",
      "fwd FLOPs:                                                              1.96 MFLOPS\n",
      "fwd+bwd MACs:                                                           2.72 MMACs\n",
      "fwd+bwd FLOPs:                                                          5.88 MFLOPS\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "INFO: \n",
      "  | Name          | Type              | Params | Mode\n",
      "-----------------------------------------------------------\n",
      "0 | criterion     | BCEWithLogitsLoss | 0      | eval\n",
      "1 | val_criterion | BCEWithLogitsLoss | 0      | eval\n",
      "2 | in_norm       | Identity          | 0      | eval\n",
      "3 | backbone      | XceptionBlockPlus | 15.3 K | eval\n",
      "4 | head          | Sequential        | 3.6 K  | eval\n",
      "-----------------------------------------------------------\n",
      "18.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 K    Total params\n",
      "0.076     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "153       Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "INFO: Metric val_loss improved. New best score: 0.986\n",
      "INFO: Epoch 0, global step 512: 'val_loss' reached 0.98641 (best 0.98641), saving model to '/kaggle/working/best_epoch=00-val_loss=0.986.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.979\n",
      "INFO: Epoch 0, global step 1024: 'val_loss' reached 0.97862 (best 0.97862), saving model to '/kaggle/working/best_epoch=00-val_loss=0.979.ckpt' as top 1\n",
      "INFO: Epoch 0, global step 1536: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.965\n",
      "INFO: Epoch 0, global step 2048: 'val_loss' reached 0.96456 (best 0.96456), saving model to '/kaggle/working/best_epoch=00-val_loss=0.965.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.040 >= min_delta = 0.0. New best score: 0.925\n",
      "INFO: Epoch 0, global step 2560: 'val_loss' reached 0.92505 (best 0.92505), saving model to '/kaggle/working/best_epoch=00-val_loss=0.925.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.915\n",
      "INFO: Epoch 0, global step 3072: 'val_loss' reached 0.91507 (best 0.91507), saving model to '/kaggle/working/best_epoch=00-val_loss=0.915.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.136 >= min_delta = 0.0. New best score: 0.779\n",
      "INFO: Epoch 1, global step 3979: 'val_loss' reached 0.77902 (best 0.77902), saving model to '/kaggle/working/best_epoch=01-val_loss=0.779.ckpt' as top 1\n",
      "INFO: Epoch 1, global step 4491: 'val_loss' was not in top 1\n",
      "INFO: Epoch 1, global step 5003: 'val_loss' was not in top 1\n",
      "INFO: Epoch 1, global step 5515: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.775\n",
      "INFO: Epoch 1, global step 6027: 'val_loss' reached 0.77537 (best 0.77537), saving model to '/kaggle/working/best_epoch=01-val_loss=0.775.ckpt' as top 1\n",
      "INFO: Epoch 1, global step 6539: 'val_loss' was not in top 1\n",
      "INFO: Epoch 2, global step 7446: 'val_loss' was not in top 1\n",
      "INFO: Epoch 2, global step 7958: 'val_loss' was not in top 1\n",
      "INFO: Epoch 2, global step 8470: 'val_loss' was not in top 1\n",
      "INFO: Epoch 2, global step 8982: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.077 >= min_delta = 0.0. New best score: 0.699\n",
      "INFO: Epoch 2, global step 9494: 'val_loss' reached 0.69870 (best 0.69870), saving model to '/kaggle/working/best_epoch=02-val_loss=0.699.ckpt' as top 1\n",
      "INFO: Epoch 2, global step 10006: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 10913: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 11425: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 11937: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 12449: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 12961: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 13473: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 14380: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 14892: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 15404: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 15916: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.141 >= min_delta = 0.0. New best score: 0.558\n",
      "INFO: Epoch 4, global step 16428: 'val_loss' reached 0.55812 (best 0.55812), saving model to '/kaggle/working/best_epoch=04-val_loss=0.558.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.558\n",
      "INFO: Epoch 4, global step 16940: 'val_loss' reached 0.55760 (best 0.55760), saving model to '/kaggle/working/best_epoch=04-val_loss=0.558.ckpt' as top 1\n",
      "INFO: Epoch 5, global step 17847: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 18359: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.310 >= min_delta = 0.0. New best score: 0.248\n",
      "INFO: Epoch 5, global step 18871: 'val_loss' reached 0.24800 (best 0.24800), saving model to '/kaggle/working/best_epoch=05-val_loss=0.248.ckpt' as top 1\n",
      "INFO: Epoch 5, global step 19383: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 19895: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.043 >= min_delta = 0.0. New best score: 0.205\n",
      "INFO: Epoch 5, global step 20407: 'val_loss' reached 0.20521 (best 0.20521), saving model to '/kaggle/working/best_epoch=05-val_loss=0.205.ckpt' as top 1\n",
      "INFO: Epoch 6, global step 21314: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 21826: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 22338: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.149\n",
      "INFO: Epoch 6, global step 22850: 'val_loss' reached 0.14853 (best 0.14853), saving model to '/kaggle/working/best_epoch=06-val_loss=0.149.ckpt' as top 1\n",
      "INFO: Epoch 6, global step 23362: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.141\n",
      "INFO: Epoch 6, global step 23874: 'val_loss' reached 0.14085 (best 0.14085), saving model to '/kaggle/working/best_epoch=06-val_loss=0.141.ckpt' as top 1\n",
      "INFO: Epoch 7, global step 24781: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.035 >= min_delta = 0.0. New best score: 0.106\n",
      "INFO: Epoch 7, global step 25293: 'val_loss' reached 0.10551 (best 0.10551), saving model to '/kaggle/working/best_epoch=07-val_loss=0.106.ckpt' as top 1\n",
      "INFO: Epoch 7, global step 25805: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 26317: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 26829: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 27341: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 28248: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 28760: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 29272: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 29784: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 30296: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 30808: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 31715: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 32227: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 32739: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 33251: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 33763: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 34275: 'val_loss' was not in top 1\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# sample model configuration\n",
    "window_size = 56\n",
    "nf = 4\n",
    "coord = True\n",
    "norm = \"Instance\"\n",
    "concat_pool = False\n",
    "adaptive_size = 20\n",
    "activation = \"relu\"\n",
    "dropout = 0.3\n",
    "ks = 8\n",
    "bottleneck = True\n",
    "bn_1st = False\n",
    "norm_act = True\n",
    "width = 6\n",
    "input_norm = False\n",
    "norm_weights = False\n",
    "\n",
    "# set seed\n",
    "L.seed_everything(seed=seed, verbose=False)\n",
    "\n",
    "# setup datasets\n",
    "train_dataset = TSDataset(train_array, train_labels, window_size=window_size)\n",
    "val_dataset = TSDataset(val_array, val_labels, window_size=window_size)\n",
    "\n",
    "# train_sampler = get_weighted_sampler(train_dataset, num_samples=num_samples_per_epoch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, sampler=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# initialize model\n",
    "model = XceptionTimePlus(\n",
    "    c_in=in_channels,\n",
    "    c_out=out_channels,\n",
    "    nf=nf,\n",
    "    coord=coord,\n",
    "    norm=norm,\n",
    "    concat_pool=concat_pool,\n",
    "    adaptive_size=adaptive_size,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    ks=ks,\n",
    "    bottleneck=bottleneck,\n",
    "    bn_1st=bn_1st,\n",
    "    norm_act=norm_act,\n",
    "    width=width,\n",
    "    input_norm=input_norm,\n",
    "    window_size=window_size,\n",
    "    norm_weights=norm_weights,\n",
    "    # fixed BaseClassifier params\n",
    "    criterion=nn.BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    val_criterion=nn.BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    optimizer=\"adam\",\n",
    "    lr=1e-3,\n",
    "    patience=40,\n",
    "    min_delta=0.,\n",
    "    checkpoint_dir=file_dir,\n",
    "    use_one_cycle=True\n",
    ").cuda()\n",
    "\n",
    "# calculate MACs and model parameters\n",
    "flops, macs, num_params = calculate_flops(\n",
    "    model=model,\n",
    "    input_shape=(1, in_channels, window_size),\n",
    "    print_results=True,\n",
    "    print_detailed=False,\n",
    "    output_as_string=False,\n",
    "    include_backPropagation=False,\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    max_time=\"00:05:00:00\",\n",
    "    log_every_n_steps=16,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    logger=TensorBoardLogger(save_dir=file_dir, name=\"lightning_logs\"),\n",
    "    val_check_interval=eval_steps,\n",
    "    enable_model_summary=True,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6941031,
     "sourceId": 11129425,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4046.448432,
   "end_time": "2025-03-27T21:34:04.603232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T20:26:38.154800",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
