{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ef52b1",
   "metadata": {
    "papermill": {
     "duration": 0.002934,
     "end_time": "2025-03-27T20:21:13.321142",
     "exception": false,
     "start_time": "2025-03-27T20:21:13.318208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Image Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1901bc43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:21:13.327768Z",
     "iopub.status.busy": "2025-03-27T20:21:13.327313Z",
     "iopub.status.idle": "2025-03-27T20:21:22.331862Z",
     "shell.execute_reply": "2025-03-27T20:21:22.330935Z"
    },
    "papermill": {
     "duration": 9.0097,
     "end_time": "2025-03-27T20:21:22.333631",
     "exception": false,
     "start_time": "2025-03-27T20:21:13.323931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting calflops\r\n",
      "  Downloading calflops-0.3.2-py3-none-any.whl.metadata (28 kB)\r\n",
      "Collecting lightning\r\n",
      "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\r\n",
      "Collecting pyts==0.12.0\r\n",
      "  Downloading pyts-0.12.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.13.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (1.4.2)\r\n",
      "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.10/dist-packages (from pyts==0.12.0) (0.60.0)\r\n",
      "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from calflops) (1.2.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from calflops) (0.29.0)\r\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from calflops) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\r\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\r\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.12.0)\r\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\r\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\r\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\r\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (5.9.5)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (0.4.5)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.12)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (3.17.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.48.0->pyts==0.12.0) (0.43.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.5->pyts==0.12.0) (2.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyts==0.12.0) (3.5.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->calflops) (1.3.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.6)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->calflops) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.5->pyts==0.12.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.5->pyts==0.12.0) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.5->pyts==0.12.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.5->pyts==0.12.0) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.5->pyts==0.12.0) (2024.2.0)\r\n",
      "Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading calflops-0.3.2-py3-none-any.whl (29 kB)\r\n",
      "Downloading lightning-2.5.1-py3-none-any.whl (818 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyts, lightning, calflops\r\n",
      "Successfully installed calflops-0.3.2 lightning-2.5.1 pyts-0.12.0\r\n",
      "Collecting tsai==0.3.9\r\n",
      "  Downloading tsai-0.3.9-py3-none-any.whl.metadata (16 kB)\r\n",
      "Downloading tsai-0.3.9-py3-none-any.whl (324 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tsai\r\n",
      "Successfully installed tsai-0.3.9\r\n"
     ]
    }
   ],
   "source": [
    "!pip install calflops lightning pyts==0.12.0\n",
    "!pip install --no-deps tsai==0.3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d078c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:55:37.887055Z",
     "start_time": "2025-03-24T11:55:33.271567Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T20:21:22.342582Z",
     "iopub.status.busy": "2025-03-27T20:21:22.342261Z",
     "iopub.status.idle": "2025-03-27T20:21:39.004316Z",
     "shell.execute_reply": "2025-03-27T20:21:39.003577Z"
    },
    "papermill": {
     "duration": 16.668483,
     "end_time": "2025-03-27T20:21:39.006026",
     "exception": false,
     "start_time": "2025-03-27T20:21:22.337543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from calflops import calculate_flops\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import math\n",
    "from fastai.layers import ConvLayer, MaxPool, AdaptiveAvgPool, Flatten, SimpleSelfAttention, SEModule, NormType, AvgPool\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.meta import delegates\n",
    "from typing import Literal\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BaseLightningModel(L.LightningModule):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self._val_loss = 0.0\n",
    "        self._test_loss = 0.0\n",
    "        self._val_batches = 0\n",
    "        self._test_batches = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        self._val_loss += self.val_criterion(output, target).item()\n",
    "        self._val_batches += 1\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        self._test_loss += self.val_criterion(output, target).item()\n",
    "        self._test_batches += 1\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "            batch, y = batch\n",
    "            return self(batch), y\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.config.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.config.lr)\n",
    "        elif self.config.optimizer == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.config.optimizer}\")\n",
    "        if self.config.use_one_cycle:\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, pct_start=0.25,\n",
    "                                                            total_steps=self.trainer.estimated_stepping_batches)\n",
    "            opt_config = {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                }\n",
    "            }\n",
    "            return opt_config\n",
    "        return optimizer\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        callbacks = []\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.config.patience,\n",
    "            min_delta=self.config.min_delta,\n",
    "            verbose=True\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "        if self.config.modelOutput is not None:\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                dirpath=self.config.modelOutput,\n",
    "                filename=\"best_{epoch:02d}-{val_loss:.3f}\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                save_last=True\n",
    "            )\n",
    "            callbacks.append(checkpoint)\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "        callbacks.append(lr_monitor)\n",
    "        return callbacks\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Compute and log validation metrics at the end of the validation epoch.\"\"\"\n",
    "        val_loss = self._val_loss / self._val_batches\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self._val_loss = 0.0\n",
    "        self._val_batches = 0\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"Compute and log test metrics at the end of the test epoch.\"\"\"\n",
    "        test_loss = self._test_loss / self._test_batches\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        self._test_loss = 0.0\n",
    "        self._test_batches = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_act(activation: Literal[\"relu\", \"leakyrelu\", \"mish\", \"silu\", \"hardswish\", \"gelu\", \"celu\", \"elu\"] = \"relu\"):\n",
    "        activation = activation.lower()\n",
    "        if activation == \"relu\":\n",
    "            return torch.nn.ReLU\n",
    "        elif activation == \"leakyrelu\":\n",
    "            return torch.nn.LeakyReLU\n",
    "        elif activation == \"mish\":\n",
    "            return torch.nn.Mish\n",
    "        elif activation == \"silu\":  # Swish\n",
    "            return torch.nn.SiLU\n",
    "        elif activation == \"hardswish\":\n",
    "            return torch.nn.Hardswish\n",
    "        elif activation == \"gelu\":\n",
    "            return torch.nn.GELU\n",
    "        elif activation == \"celu\":\n",
    "            return torch.nn.CELU\n",
    "        elif activation == \"elu\":\n",
    "            return torch.nn.ELU\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "\n",
    "class BaseClassifier(BaseLightningModel):\n",
    "    def __init__(self, criterion, val_criterion, optimizer, lr, patience, min_delta, checkpoint_dir, use_one_cycle):\n",
    "        super().__init__()\n",
    "        self.criterion = criterion if criterion is not None else nn.BCEWithLogitsLoss()\n",
    "        self.val_criterion = val_criterion if val_criterion is not None else criterion\n",
    "        self.optimizer_name = optimizer\n",
    "        self.lr = lr\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.use_one_cycle = use_one_cycle\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        elif self.optimizer_name == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.optimizer_name}\")\n",
    "        if self.use_one_cycle:\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr, pct_start=0.25,\n",
    "                                                            total_steps=self.trainer.estimated_stepping_batches)\n",
    "            opt_config = {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                }\n",
    "            }\n",
    "            return opt_config\n",
    "        return optimizer\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        callbacks = []\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=self.patience,\n",
    "            min_delta=self.min_delta,\n",
    "            verbose=True\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "        if self.checkpoint_dir is not None:\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                dirpath=self.checkpoint_dir,\n",
    "                filename=\"best_{epoch:02d}-{val_loss:.3f}\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                save_last=True\n",
    "            )\n",
    "            callbacks.append(checkpoint)\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "        callbacks.append(lr_monitor)\n",
    "        return callbacks\n",
    "\n",
    "\n",
    "class GAFTransform(nn.Module):\n",
    "    \"\"\"Transforms a batch of times eries data to a Gramian Angular Field (GAF).\"\"\"\n",
    "    def __init__(self, method: Literal[\"summation\", \"difference\"] = \"summation\", eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert method in [\"summation\", \"difference\"], \"Method must be either 'summation' or 'difference'\"\n",
    "        self.method = method\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Expects x to be of shape (batch_size, channels, seq_len)\n",
    "        Returns image tensor of shape (batch_size, channels, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        x_cos = self.min_max_scale(x)  # Min-Max scaling to range [-1, 1]\n",
    "\n",
    "        # Calculate GAF\n",
    "        x_sin = (1 - x_cos ** 2) ** 0.5\n",
    "        if self.method == \"summation\":\n",
    "            gaf = torch.einsum(\"bci,bcj->bcij\", x_cos, x_cos) - torch.einsum(\"bci,bcj->bcij\", x_sin, x_sin)\n",
    "        else:\n",
    "            gaf = torch.einsum(\"bci,bcj->bcij\", x_sin, x_cos) - torch.einsum(\"bci,bcj->bcij\", x_cos, x_sin)\n",
    "\n",
    "        gaf = gaf / 2 + 0.5  # Scale images to range [0, 1]\n",
    "        return gaf\n",
    "\n",
    "    def min_max_scale(self, x):\n",
    "        \"\"\"Min-Max scaling each sequence to range [-1, 1]\"\"\"\n",
    "        mins = x.min(dim=-1, keepdim=True).values\n",
    "        maxs = x.max(dim=-1, keepdim=True).values\n",
    "        return 2 * (x - mins) / (maxs - mins + self.eps) - 1\n",
    "\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray = None,\n",
    "            window_size: int = 1,\n",
    "            stride: int = 1,\n",
    "    ):\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.number_of_windows = ((len(X) - window_size) // stride) + 1\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).squeeze() if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get window\n",
    "        start_idx = idx * self.stride\n",
    "        end_idx = start_idx + self.window_size\n",
    "\n",
    "        # apply instance normalization\n",
    "        window = self.X[start_idx:end_idx]  # (window_size, n_features)\n",
    "\n",
    "        # change to channels first format\n",
    "        window = window.permute(1, 0)\n",
    "\n",
    "        # get label and return\n",
    "        if self.y is not None:\n",
    "            return window, self.y[end_idx - 1]\n",
    "        return window\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Resnet block from `ni` to `nh` with `stride`\"\"\"\n",
    "\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, expansion, ni, nf, stride=1, groups=1, reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n",
    "                 sa=False, sym=False, norm_type=\"Batch\", act_cls=nn.ReLU, ndim=2, ks=3,\n",
    "                 pool=AvgPool, pool_first=True, **kwargs):\n",
    "        super().__init__()\n",
    "        norm2 = (NormType.BatchZero if norm_type == \"Batch\" else\n",
    "                 NormType.InstanceZero if norm_type == \"Instance\" else norm_type)\n",
    "        if nh2 is None:\n",
    "            nh2 = nf\n",
    "        if nh1 is None:\n",
    "            nh1 = nh2\n",
    "        nf, ni = nf * expansion, ni * expansion\n",
    "        k0 = dict(norm_type=norm_type, act_cls=act_cls, ndim=ndim, **kwargs)\n",
    "        k1 = dict(norm_type=norm2, act_cls=None, ndim=ndim, **kwargs)\n",
    "        convpath = [ConvLayer(ni, nh2, ks, stride=stride, groups=ni if dw else groups, **k0),\n",
    "                    ConvLayer(nh2, nf, ks, groups=g2, **k1)\n",
    "                    ] if expansion == 1 else [\n",
    "            ConvLayer(ni, nh1, 1, **k0),\n",
    "            ConvLayer(nh1, nh2, ks, stride=stride, groups=nh1 if dw else groups, **k0),\n",
    "            ConvLayer(nh2, nf, 1, groups=g2, **k1)]\n",
    "        if reduction:\n",
    "            convpath.append(SEModule(nf, reduction=reduction, act_cls=act_cls))\n",
    "        if sa:\n",
    "            convpath.append(SimpleSelfAttention(nf, ks=1, sym=sym))\n",
    "        self.convpath = nn.Sequential(*convpath)\n",
    "        idpath = []\n",
    "        if ni != nf:\n",
    "            idpath.append(ConvLayer(ni, nf, 1, act_cls=None, ndim=ndim, **kwargs))\n",
    "        if stride != 1:\n",
    "            idpath.insert((1, 0)[pool_first], pool(stride, ndim=ndim, ceil_mode=True))\n",
    "        self.idpath = nn.Sequential(*idpath)\n",
    "        self.act = nn.ReLU() if act_cls is None else act_cls()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.convpath(x) + self.idpath(x))\n",
    "\n",
    "\n",
    "def SEBlock(expansion, ni, nf, groups=1, reduction=16, stride=1, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh1=nf * 2,\n",
    "                    nh2=nf * expansion, **kwargs)\n",
    "\n",
    "\n",
    "def SEResNeXtBlock(expansion, ni, nf, groups=32, reduction=16, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh2=w, **kwargs)\n",
    "\n",
    "\n",
    "def SeparableBlock(expansion, ni, nf, reduction=16, stride=1, base_width=4, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, reduction=reduction, nh2=nf * 2, dw=True, **kwargs)\n",
    "\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None:\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children():\n",
    "        init_cnn(l)\n",
    "\n",
    "\n",
    "class XResNet(BaseClassifier):\n",
    "    @delegates(ResBlock)\n",
    "    def __init__(self, block: str, expansion: int, layers: list, p=0.0, c_in=6, n_out=6, stem_szs=(32, 32, 64),\n",
    "                 widen=1.0, sa=False, act_cls=\"relu\", ndim=2, ks=3, stride=2, criterion=None, val_criterion=None,\n",
    "                 optimizer=\"adam\", lr=1e-3, patience=5, min_delta=0.0, checkpoint_dir=None, use_one_cycle=False, **kwargs):\n",
    "        super().__init__(criterion, val_criterion, optimizer, lr, patience, min_delta, checkpoint_dir, use_one_cycle)\n",
    "        store_attr('expansion,ndim,ks')\n",
    "        self.act_cls = self.get_act(act_cls)\n",
    "        self.block = self._get_block(block)\n",
    "        if ks % 2 == 0:\n",
    "            raise Exception('kernel size has to be odd!')\n",
    "\n",
    "        self.transform = GAFTransform(\"summation\")\n",
    "\n",
    "        # Create stem layers\n",
    "        stem_szs = [c_in, *stem_szs]\n",
    "        self.stem = nn.ModuleList([\n",
    "            ConvLayer(stem_szs[i], stem_szs[i + 1], ks=ks, stride=stride if i == 0 else 1,\n",
    "                      act_cls=self.act_cls, ndim=ndim)\n",
    "            for i in range(3)\n",
    "        ])\n",
    "\n",
    "        # Create maxpool layer\n",
    "        self.maxpool = MaxPool(ks=ks, stride=stride, padding=ks // 2, ndim=ndim)\n",
    "\n",
    "        # Create blocks\n",
    "        layer_sizes = [64, 128, 256, 512][:len(layers)]\n",
    "        block_szs = [int(o * widen) for o in layer_sizes + [256] * (len(layers) - 4)]\n",
    "        block_szs = [64 // expansion] + block_szs\n",
    "        self.blocks = nn.ModuleList(self._make_blocks(layers, block_szs, sa, stride, **kwargs))\n",
    "\n",
    "        # Create head layers\n",
    "        self.adaptive_pool = AdaptiveAvgPool(sz=1, ndim=ndim)\n",
    "        self.flatten = Flatten()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(block_szs[-1] * expansion, n_out)\n",
    "\n",
    "        init_cnn(self)\n",
    "\n",
    "    def _make_blocks(self, layers, block_szs, sa, stride, **kwargs):\n",
    "        return [self._make_layer(ni=block_szs[i], nf=block_szs[i + 1], blocks=l,\n",
    "                                 stride=1 if i == 0 else stride, sa=sa and i == len(layers) - 4, **kwargs)\n",
    "                for i, l in enumerate(layers)]\n",
    "\n",
    "    def _make_layer(self, ni, nf, blocks, stride, sa, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            *[self.block(self.expansion, ni if i == 0 else nf, nf, stride=stride if i == 0 else 1,\n",
    "                         sa=sa and i == (blocks - 1), act_cls=self.act_cls, ndim=self.ndim, ks=self.ks, **kwargs)\n",
    "              for i in range(blocks)])\n",
    "\n",
    "    def _get_block(self, block_name: str = \"resblock\"):\n",
    "        block_name = block_name.lower()\n",
    "        if block_name == \"resblock\":\n",
    "            return ResBlock\n",
    "        elif block_name == \"seblock\":\n",
    "            return SEBlock\n",
    "        elif block_name == \"seresnextblock\":\n",
    "            return SEResNeXtBlock\n",
    "        elif block_name == \"separableblock\":\n",
    "            return SeparableBlock\n",
    "        else:\n",
    "            raise ValueError(f\"Block {block_name} not recognized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert to GAF\n",
    "        x = self.transform(x)\n",
    "\n",
    "        # Apply stem layers\n",
    "        for stem_layer in self.stem:\n",
    "            x = stem_layer(x)\n",
    "\n",
    "        # Apply maxpool\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Apply blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Apply head\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_weighted_sampler(dataset: TSDataset, num_samples: int):\n",
    "    global_labels = dataset.y.numpy().max(axis=1).astype(int)\n",
    "    unique, counts = np.unique(global_labels, return_counts=True)\n",
    "    class_weights = 1.0 / counts\n",
    "    sample_weights = class_weights[global_labels]\n",
    "    sample_weights = sample_weights[dataset.window_size - 1::dataset.stride]\n",
    "    return WeightedRandomSampler(sample_weights, num_samples, replacement=True)\n",
    "\n",
    "\n",
    "def get_class_weights(labels: np.ndarray):\n",
    "    pos_counts = labels.sum(axis=0)\n",
    "    neg_counts = len(labels) - pos_counts\n",
    "    class_weights = neg_counts / pos_counts\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return torch.tensor(class_weights, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55842e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:55:48.032845Z",
     "start_time": "2025-03-24T11:55:48.023301Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T20:21:39.014928Z",
     "iopub.status.busy": "2025-03-27T20:21:39.014299Z",
     "iopub.status.idle": "2025-03-27T20:21:39.018440Z",
     "shell.execute_reply": "2025-03-27T20:21:39.017749Z"
    },
    "papermill": {
     "duration": 0.009902,
     "end_time": "2025-03-27T20:21:39.019884",
     "exception": false,
     "start_time": "2025-03-27T20:21:39.009982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_samples_per_epoch = 2_097_152  # 2048 steps\n",
    "epochs = 10\n",
    "batch_size = 1024\n",
    "accumulate_grad_batches = 2048 // batch_size  # virtual batch size of 2048\n",
    "split = 264_960\n",
    "file_dir = \"/kaggle/working\"\n",
    "\n",
    "seed = 0\n",
    "in_channels = 6\n",
    "out_channels = 6\n",
    "eval_steps = 1024  # val_check_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf4e209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T20:21:39.028478Z",
     "iopub.status.busy": "2025-03-27T20:21:39.028162Z",
     "iopub.status.idle": "2025-03-27T20:22:08.853669Z",
     "shell.execute_reply": "2025-03-27T20:22:08.852808Z"
    },
    "papermill": {
     "duration": 29.831507,
     "end_time": "2025-03-27T20:22:08.855139",
     "exception": false,
     "start_time": "2025-03-27T20:21:39.023632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [75.98615178 76.00953507 75.47281678 75.46128577 76.02624612 75.9719617 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7099201, 6), (7099201, 6), (264960, 6), (264960, 6))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/esa-mission-1-train-dataset/84_months.train.csv\")\n",
    "\n",
    "target_channels = [f\"channel_{i}\" for i in range(41, 47)]\n",
    "label_cols = [\"is_anomaly_\" + col for col in target_channels]\n",
    "\n",
    "train_array = df_train[target_channels].iloc[:-split].values.astype(np.float32)\n",
    "val_array = df_train[target_channels].iloc[-split:].values.astype(np.float32)\n",
    "train_labels = df_train[label_cols].iloc[:-split].values.astype(np.float32).clip(0., 1.)\n",
    "val_labels = df_train[label_cols].iloc[-split:].values.astype(np.float32).clip(0., 1.)\n",
    "\n",
    "class_weights = get_class_weights(train_labels)\n",
    "\n",
    "train_array.shape, train_labels.shape, val_array.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4051920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:56:28.201541Z",
     "start_time": "2025-03-24T11:56:28.175734Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T20:22:08.863943Z",
     "iopub.status.busy": "2025-03-27T20:22:08.863642Z",
     "iopub.status.idle": "2025-03-27T22:41:32.408424Z",
     "shell.execute_reply": "2025-03-27T22:41:32.407458Z"
    },
    "papermill": {
     "duration": 8363.551157,
     "end_time": "2025-03-27T22:41:32.410270",
     "exception": false,
     "start_time": "2025-03-27T20:22:08.859113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  127.86 K\n",
      "fwd MACs:                                                               45 MMACs\n",
      "fwd FLOPs:                                                              90.48 MFLOPS\n",
      "fwd+bwd MACs:                                                           135 MMACs\n",
      "fwd+bwd FLOPs:                                                          271.45 MFLOPS\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "INFO: \n",
      "  | Name          | Type              | Params | Mode\n",
      "-----------------------------------------------------------\n",
      "0 | criterion     | BCEWithLogitsLoss | 0      | eval\n",
      "1 | val_criterion | BCEWithLogitsLoss | 0      | eval\n",
      "2 | transform     | GAFTransform      | 0      | eval\n",
      "3 | stem          | ModuleList        | 47.2 K | eval\n",
      "4 | maxpool       | MaxPool2d         | 0      | eval\n",
      "5 | blocks        | ModuleList        | 79.8 K | eval\n",
      "6 | adaptive_pool | AdaptiveAvgPool2d | 0      | eval\n",
      "7 | flatten       | Flatten           | 0      | eval\n",
      "8 | dropout       | Dropout           | 0      | eval\n",
      "9 | fc            | Linear            | 774    | eval\n",
      "-----------------------------------------------------------\n",
      "127 K     Trainable params\n",
      "0         Non-trainable params\n",
      "127 K     Total params\n",
      "0.511     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "54        Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "INFO: Metric val_loss improved. New best score: 1.007\n",
      "INFO: Epoch 0, global step 512: 'val_loss' reached 1.00685 (best 1.00685), saving model to '/kaggle/working/best_epoch=00-val_loss=1.007.ckpt' as top 1\n",
      "INFO: Epoch 0, global step 1024: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.975\n",
      "INFO: Epoch 0, global step 1536: 'val_loss' reached 0.97460 (best 0.97460), saving model to '/kaggle/working/best_epoch=00-val_loss=0.975.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.971\n",
      "INFO: Epoch 0, global step 2048: 'val_loss' reached 0.97127 (best 0.97127), saving model to '/kaggle/working/best_epoch=00-val_loss=0.971.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.966\n",
      "INFO: Epoch 0, global step 2560: 'val_loss' reached 0.96598 (best 0.96598), saving model to '/kaggle/working/best_epoch=00-val_loss=0.966.ckpt' as top 1\n",
      "INFO: Epoch 0, global step 3072: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.965\n",
      "INFO: Epoch 1, global step 3979: 'val_loss' reached 0.96464 (best 0.96464), saving model to '/kaggle/working/best_epoch=01-val_loss=0.965.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.111 >= min_delta = 0.0. New best score: 0.853\n",
      "INFO: Epoch 1, global step 4491: 'val_loss' reached 0.85320 (best 0.85320), saving model to '/kaggle/working/best_epoch=01-val_loss=0.853.ckpt' as top 1\n",
      "INFO: Epoch 1, global step 5003: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 0.823\n",
      "INFO: Epoch 1, global step 5515: 'val_loss' reached 0.82324 (best 0.82324), saving model to '/kaggle/working/best_epoch=01-val_loss=0.823.ckpt' as top 1\n",
      "INFO: Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.820\n",
      "INFO: Epoch 1, global step 6027: 'val_loss' reached 0.81961 (best 0.81961), saving model to '/kaggle/working/best_epoch=01-val_loss=0.820.ckpt' as top 1\n",
      "INFO: Epoch 1, global step 6539: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.819\n",
      "INFO: Epoch 2, global step 7446: 'val_loss' reached 0.81880 (best 0.81880), saving model to '/kaggle/working/best_epoch=02-val_loss=0.819.ckpt' as top 1\n",
      "INFO: Epoch 2, global step 7958: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.813\n",
      "INFO: Epoch 2, global step 8470: 'val_loss' reached 0.81338 (best 0.81338), saving model to '/kaggle/working/best_epoch=02-val_loss=0.813.ckpt' as top 1\n",
      "INFO: Epoch 2, global step 8982: 'val_loss' was not in top 1\n",
      "INFO: Epoch 2, global step 9494: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.812\n",
      "INFO: Epoch 2, global step 10006: 'val_loss' reached 0.81242 (best 0.81242), saving model to '/kaggle/working/best_epoch=02-val_loss=0.812.ckpt' as top 1\n",
      "INFO: Epoch 3, global step 10913: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 11425: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 11937: 'val_loss' was not in top 1\n",
      "INFO: Epoch 3, global step 12449: 'val_loss' was not in top 1\n",
      "INFO: Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.804\n",
      "INFO: Epoch 3, global step 12961: 'val_loss' reached 0.80433 (best 0.80433), saving model to '/kaggle/working/best_epoch=03-val_loss=0.804.ckpt' as top 1\n",
      "INFO: Epoch 3, global step 13473: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 14380: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 14892: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 15404: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 15916: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 16428: 'val_loss' was not in top 1\n",
      "INFO: Epoch 4, global step 16940: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 17847: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 18359: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 18871: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 19383: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 19895: 'val_loss' was not in top 1\n",
      "INFO: Epoch 5, global step 20407: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 21314: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 21826: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 22338: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 22850: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 23362: 'val_loss' was not in top 1\n",
      "INFO: Epoch 6, global step 23874: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 24781: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 25293: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 25805: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 26317: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 26829: 'val_loss' was not in top 1\n",
      "INFO: Epoch 7, global step 27341: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 28248: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 28760: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 29272: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 29784: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 30296: 'val_loss' was not in top 1\n",
      "INFO: Epoch 8, global step 30808: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 31715: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 32227: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 32739: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 33251: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 33763: 'val_loss' was not in top 1\n",
      "INFO: Epoch 9, global step 34275: 'val_loss' was not in top 1\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# sample model configuration\n",
    "window_size = 88\n",
    "block = \"resblock\"\n",
    "expansion = 4\n",
    "layers = [1] * 2\n",
    "p = 0.1\n",
    "stem_szs_0 = 32\n",
    "stem_szs_1 = 8\n",
    "widen = 0.25\n",
    "sa = True\n",
    "act_cls = \"leakyrelu\"\n",
    "ks = 7\n",
    "stride = 3\n",
    "\n",
    "# set seed\n",
    "L.seed_everything(seed=seed, verbose=False)\n",
    "\n",
    "# setup datasets\n",
    "train_dataset = TSDataset(train_array, train_labels, window_size=window_size)\n",
    "val_dataset = TSDataset(val_array, val_labels, window_size=window_size)\n",
    "\n",
    "# train_sampler = get_weighted_sampler(train_dataset, num_samples=num_samples_per_epoch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, sampler=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# initialize model\n",
    "model = XResNet(\n",
    "    block=block,\n",
    "    expansion=expansion,\n",
    "    layers=layers,\n",
    "    p=p,\n",
    "    c_in=in_channels,\n",
    "    n_out=out_channels,\n",
    "    stem_szs=(stem_szs_0, stem_szs_1, 64),\n",
    "    widen=widen,\n",
    "    sa=sa,\n",
    "    act_cls=act_cls,\n",
    "    ndim=2,\n",
    "    ks=ks,\n",
    "    stride=stride,\n",
    "    # fixed BaseClassifier params\n",
    "    criterion=nn.BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    val_criterion=nn.BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    optimizer=\"adam\",\n",
    "    lr=1e-4,\n",
    "    patience=40,\n",
    "    min_delta=0.,\n",
    "    checkpoint_dir=file_dir,\n",
    "    use_one_cycle=True\n",
    ").cuda()\n",
    "\n",
    "# calculate MACs and model parameters\n",
    "flops, macs, num_params = calculate_flops(\n",
    "    model=model,\n",
    "    input_shape=(1, in_channels, window_size),\n",
    "    print_results=True,\n",
    "    print_detailed=False,\n",
    "    output_as_string=False,\n",
    "    include_backPropagation=False,\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=epochs,\n",
    "    max_time=\"00:05:00:00\",\n",
    "    log_every_n_steps=16,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    logger=TensorBoardLogger(save_dir=file_dir, name=\"lightning_logs\"),\n",
    "    val_check_interval=eval_steps,\n",
    "    enable_model_summary=True,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6941031,
     "sourceId": 11129425,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8424.841158,
   "end_time": "2025-03-27T22:41:35.257011",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T20:21:10.415853",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
